# Practical Session #1: Introduction

1. Find in news sources a general public article reporting the discovery of a software bug. Describe the bug. If possible, say whether the bug is local or global and describe the failure that manifested its presence. Explain the repercussions of the bug for clients/consumers and the company or entity behind the faulty program. Speculate whether, in your opinion, testing the right scenario would have helped to discover the fault.

2. Apache Commons projects are known for the quality of their code and development practices. They use dedicated issue tracking systems to discuss and follow the evolution of bugs and new features. The following link https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-794?filter=doneissues points to the issues considered as solved for the Apache Commons Collections project. Among those issues find one that corresponds to a bug that has been solved. Classify the bug as local or global. Explain the bug and the solution. Did the contributors of the project add new tests to ensure that the bug is detected if it reappears in the future?

3. Netflix is famous, among other things we love, for the popularization of *Chaos Engineering*, a fault-tolerance verification technique. The company has implemented protocols to test their entire system in production by simulating faults such as a server shutdown. During these experiments they evaluate the system's capabilities of delivering content under different conditions. The technique was described in [a paper](https://arxiv.org/ftp/arxiv/papers/1702/1702.05843.pdf) published in 2016. Read the paper and briefly explain what are the concrete experiments they perform, what are the requirements for these experiments, what are the variables they observe and what are the main results they obtained. Is Netflix the only company performing these experiments? Speculate how these experiments could be carried in other organizations in terms of the kind of experiment that could be performed and the system variables to observe during the experiments.

4. [WebAssembly](https://webassembly.org/) has become the fourth official language supported by web browsers. The language was born from a joint effort of the major players in the Web. Its creators presented their design decisions and the formal specification in [a scientific paper](https://people.mpi-sws.org/~rossberg/papers/Haas,%20Rossberg,%20Schuff,%20Titzer,%20Gohman,%20Wagner,%20Zakai,%20Bastien,%20Holman%20-%20Bringing%20the%20Web%20up%20to%20Speed%20with%20WebAssembly.pdf) published in 2018. The goal of the language is to be a low level, safe and portable compilation target for the Web and other embedding environments. The authors say that it is the first industrial strength language designed with formal semantics from the start. This evidences the feasibility of constructive approaches in this area. Read the paper and explain what are the main advantages of having a formal specification for WebAssembly. In your opinion, does this mean that WebAssembly implementations should not be tested? 

5.  Shortly after the appearance of WebAssembly another paper proposed a mechanized specification of the language using Isabelle. The paper can be consulted here: https://www.cl.cam.ac.uk/~caw77/papers/mechanising-and-verifying-the-webassembly-specification.pdf. This mechanized specification complements the first formalization attempt from the paper. According to the author of this second paper, what are the main advantages of the mechanized specification? Did it help improving the original formal specification of the language? What other artifacts were derived from this mechanized specification? How did the author verify the specification? Does this new specification removes the need for testing?

## Answers

1. Nous avons décidé de travailler sur le USS Yorktown Bug, un bateau qui a échoué à cause d'une division par zéro le 21 Septembre 1997. Ce bug est local car il est dû à une division par zéro et par un manque de vérification de la part de l'équipe. A cause de cette erreur, toutes les machines sur le réseau du bateau se sont arrêtées notamment le système de propulsion. L'erreur est apparue après qu'un membre de l'équipage a entré la valeur zéro dans la base de données, le système a donc renvoyé une exception qui a causé l'incident. Dans notre cas, tester la division par zéro dans le programme aurait évité le bug car c’est une opération qui peut être récurrente lors de mauvaises manipulations de la base de données.

2. Parmi les issues résolues du Apache Common Project, nous avons choisi de nous intéresser à l’issue 813 : CollectionUtils.retainAll() not throwing proper NullPointerException(NPE). Plus précisément, d’après la documentation la méthode est censée retourner une exception lorsque l’un de ses paramètres est null, or elle ne le fait pas. Ici le bug est local, il s’agit d’une erreur interne au code, qui ne provient pas de la réutilisation d’un composant, ou d’interactions hardware/software. Pour résoudre ce bug, le contributeur a simplement ajouté à tous les endroits nécessaires l’assertion Objects.requireNonNull sur le paramètre en question, ce qui empêche le mauvais fonctionnement. Il a aussi ajouté un certain nombre de tests vérifiant que son amélioration fonctionnait, et permettant d’éviter que la situation se reproduise.

3. Dans un premier temps, Netflix expérimente le Chaos Engineering avec un service interne appelé Chaos Monkey. Ce service choisit de façon aléatoire des machines virtuelles qu’il va forcer à s’éteindre pour provoquer une erreur dans le système. Les ingénieurs devront donc réagir en temps réel pour résoudre le problème. Ensuite, le Chaos Kong, un autre service beaucoup plus grand que le Chaos Monkey, exécute la même expérimentation mais cette fois sur des régions entières. Pour ces expérimentations Netflix possède Open Connect, un cache de données vidéos distribué géographiquement couplé à une grande capacité de stockage partout dans le monde. L’entreprise de streaming utilise aussi Control Plane, un groupe de services implémentant toutes les fonctionnalités de l’UI qui est exécuté dans des machines virtuelles Amazon sur plusieurs régions (Chaos Kong Test). Ces tests permettent d’observer une variable importante pour Netflix, la disponibilité. En effet, Netflix va observé le nombre d’utilisateurs lançant un streaming de vidéo chaque seconde, si ce nombre baisse énormément après un test, c’est qu’il y a une grosse erreur dans le système. Plusieurs grosses entreprises comme Amazon, Google, Microsoft et Facebook ont commencé à utiliser le même système d’expérimentation. Amazon pourrait observer le nombre d’achats chaque secondes, Google pourrait calculer le nombre de recherches et Facebook comparer le nombre de posts et réactions.

4. Le fait d’avoir décrit les spécifications et la sémantique du programme de manière formelle avant même de débuter le développement fait que l’outil a un design très simple et clair. Cela le rend plus facile d’utilisation, et permet aussi une validation et une compilation plus rapide et plus simple. Cela le rend aussi plus sécurisé et plus compact. Cependant cela n’empêche pas de faire des tests, en effet même si toutes les fonctionnalités du programme sont bien spécifiées, il peut toujours y avoir des erreurs, peut-être dues à l’utilisateur lui-même. L’avantage est qu’une spécification formelle permet de réaliser des tests plus efficaces, car on sait précisément les résultats attendus.

5. La mécanisation Isabelle a permis de mécaniser et vérifier les nouvelles fonctionnalités ajoutées à la spécification. Cela a permis de révéler des problèmes inhérents au langage WebAssembly, comme par exemple une vérification de type pas toujours solide. De cette mécanisation ont dérivé plusieurs artéfacts comme un type checker et un interpréteur de vérification exécutable agissant séparément. Avec ces artéfacts et ses preuves de corrections,en utilisant la suite de tests de conformité de WebAssembly et en menant des expériences de fuzzing, l’auteur a pu validé sa spécification de mécanisation.Comme pour la question 4, même si la spécification s’est encore améliorée avec la mécanisation, il y a toujours des risques d’erreurs, il ne faut pas se reposer uniquement sur une spécification, et continuer à tester son programme. 
